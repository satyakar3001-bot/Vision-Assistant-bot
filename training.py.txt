from glob import glob
import cv2
data_path = '../input/flickr8k-sau/Flickr_Data/Images/' ##data has been taken from kaggle 
images_paths = glob(data_path+'*.jpg')
import tensorflow as tf
from tensorflow import keras
from keras.applications import ResNet50
resnet_model_tf = ResNet50(include_top=True)
from keras.models import Model
end = resnet_model_tf.layers[-2].output
final_resnet_model_tf = Model(inputs = resnet_model_tf.input,outputs = end)

object_classes = {} # resnet50 classification of objects by transfer learning
count = 0
for i in images_paths:
    upperbound = 1500
    lowerbound = 0
    if count<upperbound and count>lowerbound:   # and count<3000:        
        img = cv2.imread(i)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (224,224))

        img = img.reshape(1,224,224,3)
        pred = final_resnet_model_tf.predict(img).reshape(2048,)        
        img_name = i.split('/')[-1]    
        object_classes[img_name] = pred    
      
    if count > upperbound:                         ##taking batch of 100 for testing
        break
    if count % 100==0:
        print(count)
        
    count += 1  

text_data_path = '../input/flickr8k-sau/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'
texts = open(text_data_path, 'rb').read().decode('utf-8').split('\n')
text_dict = {}
for i in texts:
    try:
        img_name = i.split('\t')[0][:-2] 
        text = i.split('\t')[1]
        if img_name in object_classes:
            if img_name not in text_dict:
                text_dict[img_name] = [text]
                
            else:
                text_dict[img_name].append(text)
            
    except:
        pass
    
def preprocessing(txt):
    processed = txt.lower()
    processed = ' start ' + processed + ' end '
    return processed

for key,words in text_dict.items():
    for word in words:
        text_dict[key][words.index(word)] = preprocessing(word)
        
words_count = {}
for key,items in text_dict.items():
    for word in items:
        for word in word.split():
            if word not in words_count:

                words_count[word] = 0

            else:
                words_count[word] += 1
                
count = 1
new_dict = {}
for key,items in words_count.items():
    if words_count[key] > -1:
        new_dict[key] = count
        count += 1 
        
new_dict['<OUT>'] = len(new_dict)
for key, items in text_dict.items():
    for words in items:
        encoded = []
        for word in words.split():  
            if word not in new_dict:
                encoded.append(new_dict['<OUT>'])
            else:
                encoded.append(new_dict[word])


        text_dict[key][items.index(words)] = encoded



MAX_LEN = 36
Batch_size = 5000
vocab_size = len(new_dict)          
from keras.utils import to_categorical
from keras.preprocessing.sequence import pad_sequences
def generator(image, text): ### Function to generate 
    n_samples = 0           ### 1. padding of texts for training
                            ### 2. data to establish relation between the words
    X = []
    y_in = []
    y_out = []
    
    for key, words in text.items():
        for word in words:
            for i in range(1, len(word)):
                X.append(image[key])

                seq= [word[:i]]
                next_seq = word[i]

                seq = pad_sequences(seq, maxlen=MAX_LEN, padding='pre', truncating='pre')[0]
                next_seq = to_categorical([next_seq], num_classes=vocab_size)[0]

                y_in.append(seq)
                y_out.append(next_seq)
            
    return X, y_in, y_out

X, y_in, y_out = generator(object_classes, text_dict)

import numpy as np
X = np.array(X)
y_in = np.array(y_in, dtype='float64')
y_out = np.array(y_out, dtype='float64')
#

from keras.utils import plot_model
from keras.models import Model, Sequential
from keras.layers import Input
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Embedding
from keras.layers import Dropout
from keras.layers.merge import add
from keras.callbacks import ModelCheckpoint
from keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate
from keras.models import Sequential, Model
 
 
embedding_size = 128
max_len = MAX_LEN
vocab_size = len(words_count)+1
 
image_model = Sequential()
 
image_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))
image_model.add(RepeatVector(max_len))
 
#image_model.summary()
 
language_model = Sequential()
 
language_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))
language_model.add(LSTM(256, return_sequences=True))
language_model.add(TimeDistributed(Dense(embedding_size)))
 
#language_model.summary()
 
conca = Concatenate()([image_model.output, language_model.output])
x = LSTM(128, return_sequences=True)(conca)
x = LSTM(512, return_sequences=False)(x)
x = Dense(vocab_size)(x)
out = Activation('softmax')(x)
model = Model(inputs=[image_model.input, language_model.input], outputs = out)
 
# model.load_weights("../input/model_weights.h5")
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
#model.summary()
plot_model(model,show_shapes = True)

model.fit([X, y_in], y_out, batch_size=512, epochs=50)
model.save("adam_0.78_acc.h5")

import json
json_object = json.dumps(new_dict)
with open('decoder.json', 'w') as f:
	f.write(json_object)
	f.close()

json_object = json.dumps(inv_dict)
with open('encoder.json', 'w') as f:
	f.write(json_object)
	f.close()
